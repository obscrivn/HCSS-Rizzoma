# text <-gsub("-\\s+", "", text)
textum[i] <- data #text
}
return(textum)
}
extractContent(x)
extractContent <- function (x){
# num <-length(x$name)
num=1
textum <- vector()
for (i in 1:num) {
#  uris.name <- x$datapath[i]
uris.name=x
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
#  for (i in 1:length(data)){
#   text<-data[i]
#  }
# text <- paste(data, collapse = " ")
# text <-gsub("-\\s+", "", text)
textum[i] <- data #text
}
return(textum)
}
extractContent(x)
extractContent <- function (x){
# num <-length(x$name)
num=1
textum <- vector()
text<-vector()
for (i in 1:num) {
#  uris.name <- x$datapath[i]
uris.name=x
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
for (i in 1:length(data)){
text<-data[i]
}
# text <- paste(data, collapse = " ")
# text <-gsub("-\\s+", "", text)
textum[i] <- text
}
return(textum)
}
extractContent(x)
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = FALSE)
uris.name=x
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
data
length(data)
text<-vector()
for (y in 1:length(data)){
text[y]<-data
}
text
data[2]
data[10]
extractContent <- function (x){
# num <-length(x$name)
num=1
textum <- list()
text<-vector()
for (i in 1:num) {
#  uris.name <- x$datapath[i]
uris.name=x
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
#  for (y in 1:length(data)){
#   text[y]<-data
#  }
# text <- paste(data, collapse = " ")
# text <-gsub("-\\s+", "", text)
textum[i] <- data
}
return(textum)
}
extractContent(x)
textum[i] <- data
textum <- list()
textum[1] <- data
textum <- c(textum,data)
textum
shiny::runApp('R/Tweeter/TextMining')
?HTML
runApp('R/Tweeter/TextMining')
?read
?readPlain
?scan
y<-file.choose()
num <-c(x,y)
num
uris.name<-num[1]
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = TRUE)
data=enc2utf8(text.scan)
data
class(data)
length(data)
text <-paste(data,sep="\n")
text
textum <- list()
textum[1] <-text
textum
for (y in 1:length(data)) {
text <-c(text,data[y])
}
text
for (y in 1:length(data)) {
text <-list(text,data[y])
}
text
as.list(data)
as.vector(data)
textum[[1] <-text
textum[[1]] <-text
textum
data=enc2utf8(text.scan)
textum[[1]] <-data
textum
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
?paset
?paste
runApp('R/Tweeter/TextMining')
paste(textum,collapse="\n")
paste(textum,"\n")
paste(textum,sep="\n")
paste(textum,sep="\n")
paste(textum,collapse="\n")
data1<-paste(data,sep="\n")
data1
textum[[1]] <-data1
textum
runApp('R/Tweeter/TextMining')
?paste0
runApp('R/Tweeter/TextMining')
text.scan <- scan(uris.name, what="character", sep="",blank.lines.skip = TRUE)
text.scan
?scan
data1<-paste(data,"\n")
data1
runApp('R/Tweeter/TextMining')
textum
textum <- list()
textum[[1]] <-data1
textum
textum[[1]] <-data
textum
runApp('R/Tweeter/TextMining')
textum <- vector()
textum[[i]] <-data
textum[[1]] <-data
textum[1] <-data
textum
textum[1] <-data[1]
textum[2] <- data[2]
textum
text[[1]] <- textum
text
text[[1]] <- data
text
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
length(textum)
length(textum[[1]])
textum
data=enc2utf8(text.scan)
textum[[i]] <-data
textum[[1]] <-data
textum <- list()
textum[[1]] <-data
length(textum[[1]])
print(textum[[1]][1])
print(textum[[1]][2])
print(textum[[1]])
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = TRUE)
data=enc2utf8(text.scan)
textum[[1]] <-data
print(textum[[1]])
print(textum[[1]][1])
runApp('R/Tweeter/TextMining')
?HTML
runApp('R/Tweeter/TextMining')
?lapply
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
data1<-paste(data," ")
data1
data1<-paste(data,sep=" ")
data1
data1<-paste(data,collapse=" ")
data1
data1 <- sub("-\\s+","-",data1)
data1
textum[[1]] <-data1
textum
textum <- unlist(textum)
textum
data1 <- sub("\\s+","\\s",data1)
data1
data1<- sub("\\s+"," ",data1)
data1
data1<- sub("\\s+","",data1)
data1
data1<-paste(data,collapse=" ")
data1 <- gsub("-\\s+","-",data1)
data1<- gsub("\\s\\s+","",data1)
data1
data
data1<-paste(data,collapse=" ")
data1
data1<- gsub("\\s\\s+"," ",data1)
data1
runApp('R/Tweeter/TextMining')
shiny::runApp('R/Tweeter/TextMining')
zot.file<-file.choose()
zot.lines <- readLines(zot.file)
zot.lines <- zot.lines[grepl("<rdf:resource", zot.lines)]
zot.lines <- zot.lines[grepl(".pdf\"", zot.lines)]
zot.line.parser <- function(z){
#This takes one of our rdfresource lines and returns the pdf title
#This relies on the rdf:resource tag having exactly two double quotes
#Seems safe enough, but just to be safe we take the first and last quote mark
#This avoids issues with quotes in a title/file name.
require(stringr)
first <- 1
last <- dim(str_locate_all(z,'"')[[1]])[1]
start <- str_locate_all(z,'"')[[1]][first] + 1
stop <- str_locate_all(z[1],'"')[[1]][last] -1
substr(z, start, stop)
}
zot.pdf <- unlist(lapply(zot.lines, zot.line.parser))
zot.pdf
pattern1 <- "law"
text.extract <- list()
setwd("~/Documents/MCS/TextMiningClass/Project/TestingFiles")
num <- 1
uris.name <- z
z <- "files/137482/Giliker - 2015 - The Influence of Eu and European Human Rights Law .pdf"
uris.name <- z
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = TRUE)
texts <- enc2utf8(text.scan)
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
read.file <- tempPDF$content
texts <- enc2utf8(text.scan)
if (grepl(p1, texts)) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[i]] <-text.space
}
if (grepl(pattern1, texts)) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[i]] <-text.space
}
if (grep(pattern1, texts,grep(pattern, x, ignore.case = TRUE)) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[i]] <-text.space
}
if (grep(pattern1, texts,grep(pattern, x, ignore.case = TRUE)) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[1]] <-text.space
}
if (grep(pattern1, texts,grep(pattern, x, ignore.case = TRUE))) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[1]] <-text.space
}
if (grep(pattern1, texts, ignore.case = TRUE)) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[1]] <-text.space
}
grep(pattern1, texts, ignore.case = TRUE)
texts
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
read.file <- tempPDF$content
texts <- enc2utf8(read.file)
if (grep(pattern1, texts, ignore.case = TRUE)) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[1]] <-text.space
}
text.extract
pattern1 <- "tweet"
text.extract <- list()
if (grep(pattern1, texts, ignore.case = TRUE)) {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[1]] <-text.space
}
text.extract
pattern2 <- "human"
if ((grep(pattern1, texts, ignore.case = TRUE)) &&(grep(pattern2, texts, ignore.case = TRUE)))  {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[1]] <-text.space
}
text.extract
p1p2 <- grepl(pattern1, texts, ignore.case = TRUE) & grepl(pattern2, texts, ignore.case = TRUE)
p1p2
pat1 <- grep(pattern1, texts, ignore.case = TRUE)
pat1
pattern1 <- "law"
pat1 <- grep(pattern1, texts, ignore.case = TRUE)
pat1
pat1 <- grepl(pattern1, texts, ignore.case = TRUE)
pat1
pat1 <- grepl(pattern2, texts, ignore.case = TRUE)
pat2 <- grepl(pattern2, texts, ignore.case = TRUE)
pat2
pat1 <- grepl(pattern1, texts, ignore.case = TRUE)
pat2==TRUE
if (pat1==TRUE) &&(pat2==TRUE)  {
text.collapse<-paste(texts,collapse=" ")
}
if((pat1==TRUE) &&(pat2==TRUE))  {
text.collapse<-paste(texts,collapse=" ")
}
text.collapse
runApp('~/R/Tweeter/TextMining')
runApp('~/R/Tweeter/TextMining')
?readLines
zot.file
zot.file$datapath
#zot.lines <- readLines(zot.file)
runApp('~/R/Tweeter/TextMining')
runApp('~/R/Tweeter/TextMining')
uris.name <- "files/137482/Giliker - 2015 - The Influence of Eu and European Human Rights Law .pdf"
zot.lines <- readLines(uris.name)#zot.file)
zot.lines <- zot.lines[grepl("<rdf:resource", zot.lines)]
zot.lines <- scan(uris.name, what="character", sep="\n",blank.lines.skip = TRUE)
zot.lines
zot.lines <- readLines(uris.name)#zot.file)
zot.lines
uris.name <- "EuropeanInfluencecopy.rdf"
zot.lines <- readLines(uris.name)#zot.file)
zot.lines
zot.lines <- zot.lines[grepl("<rdf:resource", zot.lines)]
zot.lines <- zot.lines[grepl(".pdf\"", zot.lines)]
zot.line.parser <- function(z){
#This takes one of our rdfresource lines and returns the pdf title
#This relies on the rdf:resource tag having exactly two double quotes
#Seems safe enough, but just to be safe we take the first and last quote mark
#This avoids issues with quotes in a title/file name.
require(stringr)
first <- 1
last <- dim(str_locate_all(z,'"')[[1]])[1]
start <- str_locate_all(z,'"')[[1]][first] + 1
stop <- str_locate_all(z[1],'"')[[1]][last] -1
substr(z, start, stop)
}
zot.pdf <- unlist(lapply(zot.lines, zot.line.parser))
}
zot.line.parser <- function(z){
#This takes one of our rdfresource lines and returns the pdf title
#This relies on the rdf:resource tag having exactly two double quotes
#Seems safe enough, but just to be safe we take the first and last quote mark
#This avoids issues with quotes in a title/file name.
require(stringr)
first <- 1
last <- dim(str_locate_all(z,'"')[[1]])[1]
start <- str_locate_all(z,'"')[[1]][first] + 1
stop <- str_locate_all(z[1],'"')[[1]][last] -1
substr(z, start, stop)
}
zot.pdf <- unlist(lapply(zot.lines, zot.line.parser))
zot.pdf
z=zot.pdf
num <-length(z)
text.extract <- list()
for (i in 1:num) {
uris.name <- z
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
if (length(tempPDF$content)>0){
read.file <- tempPDF$content
texts <- enc2utf8(read.file)
pat1 <- grepl(pattern1, texts, ignore.case = TRUE)
pat2 <- grepl(pattern2, texts, ignore.case = TRUE)
# p1p2 <- grepl(pattern1, texts, ignore.case = TRUE) & grepl(pattern2, texts, ignore.case = TRUE)
if((pat1==TRUE) &&(pat2==TRUE))  {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[i]] <-text.space
}
}
text.extract <- unlist(text.extract)
}
z
num
text.extract <- list()
for (i in 1:num) {
uris.name <- z[i]
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
if (length(tempPDF$content)>0){
read.file <- tempPDF$content
texts <- enc2utf8(read.file)
pat1 <- grepl(pattern1, texts, ignore.case = TRUE)
pat2 <- grepl(pattern2, texts, ignore.case = TRUE)
# p1p2 <- grepl(pattern1, texts, ignore.case = TRUE) & grepl(pattern2, texts, ignore.case = TRUE)
if((pat1==TRUE) &&(pat2==TRUE))  {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[i]] <-text.space
}
}
text.extract <- unlist(text.extract)
# return(text.extract)
}
text.extract
pattern1 <- "law"
pattern2 <- "human"
text.extract <- list()
for (i in 1:num) {
uris.name <- z[i]
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
if (length(tempPDF$content)>0){
read.file <- tempPDF$content
texts <- enc2utf8(read.file)
pat1 <- grepl(pattern1, texts, ignore.case = TRUE)
pat2 <- grepl(pattern2, texts, ignore.case = TRUE)
# p1p2 <- grepl(pattern1, texts, ignore.case = TRUE) & grepl(pattern2, texts, ignore.case = TRUE)
if((pat1==TRUE) &&(pat2==TRUE))  {
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[i]] <-text.space
}
}
text.extract <- unlist(text.extract)
# return(text.extract)
}
uris.name <- z[1]
uris.name
uris.name <- z$datapath[1]
uris.name <- z[1]
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
uris.name
z="Auby - 2014 - About Europeanization of Domestic Judicial Review.pdf"
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
zotero <- function(x) {
uris.name <- "EuropeanInfluencecopy.rdf"
#uris.name <- x
#  zot.lines <- scan(uris.name, what="character", sep="\n",blank.lines.skip = TRUE)
#zot.file <-x$datapath# "/Users/majdavis/Downloads/European Influence/EuropeanInfluence.rdf"
#zot.file<-file.choose()
#Get all the lines of interest in the file
zot.lines <- readLines(uris.name)#zot.file)
zot.lines <- zot.lines[grepl("<rdf:resource", zot.lines)]
zot.lines <- zot.lines[grepl(".pdf\"", zot.lines)]
zot.line.parser <- function(z){
#This takes one of our rdfresource lines and returns the pdf title
#This relies on the rdf:resource tag having exactly two double quotes
#Seems safe enough, but just to be safe we take the first and last quote mark
#This avoids issues with quotes in a title/file name.
require(stringr)
first <- 1
last <- dim(str_locate_all(z,'"')[[1]])[1]
start <- str_locate_all(z,'"')[[1]][first] + 1
stop <- str_locate_all(z[1],'"')[[1]][last] -1
substr(z, start, stop)
}
zot.pdf <- unlist(lapply(zot.lines, zot.line.parser))
}
zotero("EuropeanInfluencecopy.rdf")
zot.pdf
t <- zotero("EuropeanInfluencecopy.rdf")
z <- zotero("EuropeanInfluencecopy.rdf")
num <-length(z)
text.extract <- list()
uris.name <- z[1]
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
read.file <- tempPDF$content
texts <- enc2utf8(read.file)
texts
getwd()
z="/Users/olgascrivner/Documents/MCS/TextMiningClass/Project/TestingFiles/files/137484/Auby - 2014 - About Europeanization of Domestic Judicial Review.pdf"
text.extract <- list()
uris.name <- z
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
z="Auby - 2014 - About Europeanization of Domestic Judicial Review.pdf"
uris.name <- z
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
setwd("~/Documents/MCS/TextMiningClass/Project/TestingFiles/files/137484")
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
z=as.character("Auby - 2014 - About Europeanization of Domestic Judicial Review.pdf")
uris.name <- z
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
tempPDF
tempPDF$meta
z
uris.name <- z
tempPDF <- readPDF(control = list(text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
tempPDF <- readPDF(control = list(info="-f",text = "-layout"))(elem = list(uri = uris.name),language="en",id="id1")
read.file <- tempPDF$content
texts <- enc2utf8(read.file)
texts
runApp('~/R/Tweeter/TextMining')
runApp('~/R/Tweeter/TextMining')
